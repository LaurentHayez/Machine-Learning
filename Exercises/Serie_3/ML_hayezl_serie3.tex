\documentclass[fontsize=12pt, usenames, dvipsnames, headinclude, headsepline, footinclude, footsepline]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx, wrapfig}
\usepackage{lmodern}
\usepackage{color, colortbl}
\usepackage{xcolor}
\usepackage{amsmath, amssymb, mathrsfs, amsthm, thmtools, MnSymbol}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{pgf, pgfplots, tikz, pst-solides3d}
\usepackage{scrlayer-scrpage}  % header and footer for KOMA-Script
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{listings}
\usepackage[inline]{enumitem}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[english]{babel}


\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}} 
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\T}{\mathcal{T}}
\renewcommand{\P}{\mathbb{P}}
\renewcommand{\S}{\mathbb{S}}
\newcommand{\bw}{\bigwedge}
\newcommand{\Fa}{\F(A)} 
\newcommand{\C}{\mathbb{C}}
\newcommand{\K}{\mathbb{K}}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}
\renewcommand{\emph}{\textbf}
\newcommand{\im}{\mathrm{im}}


\synctex=1


%%%%%%%%	Définitions des environnements de théorèmes	%%%%%%%%
%----- ENVIRONNEMENT POUR LES EXERCICES ----%
\declaretheoremstyle[
  spaceabove=0pt, spacebelow=0pt, headfont=\normalfont\bfseries\scshape,
    notefont=\mdseries, notebraces={(}{)}, headpunct={. }, headindent={},
    postheadspace={ }, postheadspace=4pt, bodyfont=\normalfont\itshape
]{defstyle}

\declaretheorem[style=defstyle, title=Exercise]{exo}
%________________________________________________________



%----- ENVIRONNEMENT POUR LES PREUVES ----%
\declaretheoremstyle[
  spaceabove=0pt, spacebelow=0pt, headfont=\normalfont\bfseries\scshape,
    notefont=\mdseries, notebraces={(}{)}, headpunct={. }, headindent={},
    postheadspace={ }, postheadspace=4pt, bodyfont=\normalfont, 
    mdframed={
      leftmargin=15,
      rightmargin=15,
      hidealllines=true,
      font=\small
   }
]{preuvestyle}

\declaretheorem[style=preuvestyle, numbered = no, title=Solution, qed=\qedsymbol]{sol}

%________________________________________________________


\addtokomafont{disposition}{\normalfont\bfseries}

\title{\normalfont{\bfseries{Machine Learning: Homework 3}}}
\author{Laurent \textsc{Hayez}}
\date{\today}

\clearpairofpagestyles                 % deletes header/footer
\pagestyle{scrheadings}           % use following definitions for header/footer
% definitions/configuration for the header
\rohead[Université de \textsc{Neuchâtel}]{Université de \textsc{Neuchâtel}}
\rehead[Université de \textsc{Neuchâtel}]{Université de \textsc{Neuchâtel}}        % equal page, right position (inner) 
\lohead[Laurent \textsc{Hayez}]{Laurent \textsc{Hayez}}        % odd   page, left  position (inner) 
\lehead[Laurent \textsc{Hayez}]{Laurent \textsc{Hayez}} % equal page, left (outer) position
% definitions/configuration for the footer
\lefoot[Machine Learning: Homework 3]{Machine Learning: Homework 3}
\lofoot[Machine Learning: Homework 3]{Machine Learning: Homework 3}
\refoot[page \pagemark]{page \pagemark}
\rofoot[page \pagemark]{page \pagemark}


\begin{document}


\renewcommand{\labelitemi}{\textbullet}



\maketitle




\begin{exo}
  Invent the two characters Taylor and Robin each with a weight in the range of [63-74] kg and a shoe size in
  the range of [40-44]. Decide (by hand) with Naïve Bayes using the probability density function whether your
  Taylor and Robin are female or male based on the following statistics.
\end{exo}

  \begin{sol}
    As we have to deal with numerical attributes, we need to use a probability density function 
    \[ f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} \]
    where $\mu$ is the sample mean and $\sigma$ is the standard deviation.

    In Table 1, we represented the weight and shoe size according to whether the person is male or female, and
    we computed the sample means and standard deviations. 

    Let $f_1$ be a gaussian distribution with $\mu = \mu_1$ and $\sigma = \sigma_1$ representing the
    distribution of the weight for the males. Let $g_1$ be the same but with $\mu = \mu_2$ and $\sigma =
    \sigma_2$ representing the distribution of the weight for the females. In the same fashion we define $f_2$
    and $g_2$ corresponding to the shoe size. Moreover let $L[\cdot]$ denote the likelihood of an event.

    If Taylor weights $73$ kgs and has shoe size 41, and Robin weights $67$ kgs and has shoe size $44$, then
    define \[E_1 = \{\text{weight = 73, shoe size = 41}\}\] and \[E_2 = \{\text{weight = 67, shoe size =
      44}\}.\] 
    We have
    \begin{align*}
      L[\text{Taylor = male} \mid E_1] &= f_1(73) \cdot f_2(41) \cdot \frac{1}{2} & \text{where }
                                                                                    \P[\text{male}] =
                                                                                    \frac{1}{2}\\
                                       &= 0.0505 \cdot 0.0118 \cdot \frac{1}{2}\\
      &= 0.00029
    \end{align*}
    Similarly, we have 
    \[ L[\text{Taylor = female} \mid E_1] = g_1(73) \cdot g_2(41) \cdot \frac{1}{2} = 0.000019.\]
    Hence we can compute the probabilities, knowing that $\P[E_1] = 0.00029 + 0.000019 = 0.000318$.
    \[ \P[\text{Taylor = male} \mid E_1] = \frac{L[\text{Taylor = male} \mid E_1]}{\P[E]} = 0.939, \]
    \[ \P[\text{Taylor = female} \mid E_1] = \frac{L[\text{Taylor = female} \mid E_1]}{\P[E]} = 0.0607. \]
    
    Applying the same reasoning for Robin gives us $L[\text{Robin = male} \mid E_2] = 0.00074$ and
    $L[\text{Robin = female} \mid E_2] = 3.112 \cdot 10^{-6}$, hence $\P[E_2] = 0.00074 + 3.112 \cdot
    10^{-6}$. 
    \[\P[\text{Robin = male} \mid E_2] = \frac{L[\text{Robin = male} \mid E_2]}{\P[E_2]} = 0.9958,\]
    \[\P[\text{Robin = female} \mid E_2] = \frac{L[\text{Robin = female} \mid E_2]}{\P[E_2]} = 0.0041.\]
  \end{sol}

\begin{table}[]
\centering
\caption{Table of values for the weights and shoe sizes for men and women}
\label{table:1}
\begin{tabular}{@{}l|l|l|l@{}}
  \toprule
  \multicolumn{2}{c|}{Weight} & \multicolumn{2}{c}{Shoe size} \\ \midrule
  Male & Female & Male  & Female \\ \midrule
  \begin{tabular}[c]{@{}l@{}}82, 80,\\ 77, 72\end{tabular} 
                               & \begin{tabular}[c]{@{}l@{}}60, 64, \\ 59, 65 \end{tabular} 
                               & \begin{tabular}[c]{@{}l@{}}45, 45, \\ 43, 43 \end{tabular} 
                               & \begin{tabular}[c]{@{}l@{}}39, 41, \\ 40, 41\end{tabular} \\ \midrule
  \begin{tabular}[c]{@{}l@{}}$\mu_1  = 77.75$\\ $\sigma_1 = 4.343$\end{tabular} 
                               & \begin{tabular}[c]{@{}l@{}}$\mu_2  = 62$\\ $\sigma_2 = 2.943$\end{tabular}  
                               & \begin{tabular}[c]{@{}l@{}}$\mu_3  = 44$\\ $\sigma_3 = 1.154$\end{tabular}
                               & \begin{tabular}[c]{@{}l@{}}$\mu_4  = 40.25$\\ $\sigma_4 = 0.957$\end{tabular}
  \\ \bottomrule
\end{tabular}
\end{table}



\begin{exo}
  Build a decision tree (by hand) based on the following data and using information theory. Then use WEKA to
  see if you get the same decision tree (ID3). Use this tree to decide if Edward, the senior student, would
  buy a new PC with his high income and good credit rating.
\end{exo}

  \begin{sol}
    To build the decision tree, we look at each attribute and compute the information gain. The initial
    information is given by $I(9/14, 5/14) = -\frac{9}{14} \log_2 \left(\frac{9}{14}\right) - \frac{5}{14}
    \log_2 \left(\frac{5}{14}\right) = 0.940$.
    \begin{description}
    \item[Age:]
      \begin{description}
      \item[youth:] We have 3 no and 2 yes, hence $I(2/5, 3/5) = -\frac{2}{5} \log_2 \left(\frac{2}{5}\right) -
        \frac{3}{5} \log_2 \left(\frac{3}{5}\right) = 0.971$ bits.
      \item[Middle-aged:] We have 4 yes, hence $I(1, 0) = 0$ bits.
      \item[Senior:] We have 3 yes and 2 no, hence $I(3/5, 2/5) = 0.971$ bits.
      \end{description}
      With these values, we can compute the information that ``Age'' gives us and the information gain. We
      have $\mathrm{Info}(Age) = \frac{5}{14} \cdot 0.971 + \frac{4}{14} \cdot 0 + \frac{5}{14} \cdot 0.971 =
      0.693$ bits. Hence
      \[\mathrm{gain}(Age) = 0.940 - 0.693 = 0.247.\]
    \end{description}
    Applying the exact same method to the attributes ``Income'', ``Student'' and ``Credit rating'', we obtain 
    \[\mathrm{gain}(Income) = 0.015\]
    \[\mathrm{gain}(Student) = 0.359\]
    \[\mathrm{gain}(Credit\ rating) = 0.048.\]
    Hence the attribute Student gives us the most information, so it will be the first node of our decision
    tree.

    We need to compute the information gain of ``Age'', ``Income'' and ``Credit rating'' given that
    Student=yes on the one hand, and Student=no on the other hand. The new initial information when Student=yes is $I(8/9, 1/9)
    = 0.503$ bits. We have
    \[\mathrm{gain}(Age) = 0.048\]
    \[\mathrm{gain}(Income) = 0.048\]
    \[\mathrm{gain}(Credit\ rating) = 0.197.\]
    Hence the second node of the decision tree will be given by the Credit rating. We keep on computing the
    information gain with the same method when Student=yes and Credit rating=excellent, because now we know
    that if the person is a student and has a good credit rating, he will buy the computer.

    We need to apply the same strategy when Student=no. In the end we computed the following decision tree:
    \begin{center}
      \begin{tikzpicture}
        \node[draw, rectangle] (S) at (0,0) {Student};
        \node[draw, rectangle] (C) at (4,2) {Credit rating};
        \node[draw, rectangle] (A1) at (8, 4) {Age};
        \node[draw, rectangle] (A2) at (4, -2) {Age};
        \node (Y1) at (8, 2) {Yes};
        \node (N1) at (12, 5) {No};
        \node (Y2) at (12, 4) {Yes};
        \node (Y3) at (12, 3) {Yes};
        \node (N2) at (8, -1) {No};
        \node (Y4) at (8, -2) {Yes};
        \node (N3) at (8, -3) {No};
        \draw[->, >=latex] (S.north east) to node[midway, above left]{Yes} (C);
        \draw[->, >=latex] (S.south east) to node[midway, below left]{No} (A2);
        \draw[->, >=latex] (C.north east) to node[midway, above left]{Excellent} (A1);
        \draw[->, >=latex] (C) to node[midway, above]{Good} (Y1);
        \draw[->, >=latex] (A1.north east) to node[midway, above left]{Senior} (N1);
        \draw[->, >=latex] (A1.east) to node[midway, above, scale=0.8]{Middle-aged} (Y2);
        \draw[->, >=latex] (A1.south east) to node[midway, below left]{Youth} (Y3);
        \draw[->, >=latex] (A2.north east) to node[midway, above left]{Senior} (N2);
        \draw[->, >=latex] (A2.east) to node[midway, above, scale=0.8]{Middle-aged} (Y4);
        \draw[->, >=latex] (A2.south east) to node[midway, below left]{Youth} (N3);
      \end{tikzpicture}
    \end{center}
    We conclude that, as Edward is a student and has a good credit rating, he would by the PC.

    From the file \texttt{ML\_hayezl\_computerRelation.arff} we computed the decision tree in WEKA with ID3 and
    we obtained the same decision tree (result is saved in the file
    \texttt{ML\_hayezl\_computerRelationResults.txt}). Using the prediction with WEKA, we also obtained that
    Edward will buy the PC.
  \end{sol}




	
\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t 
%%% End: